{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a29f74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "938bb144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 配置类 ======================\n",
    "class Config:\n",
    "    # 设备配置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 数据参数\n",
    "    seq_len = 24 * 7  # 一周数据 (小时级)\n",
    "    input_dim = 12  # 特征维度: wind, solar, electric, cooling, heating, temperature, day_cos, day_sin, hour_cos, hour_sin, mean, std\n",
    "    noise_dim = 32  # 噪声维度\n",
    "    latent_dim = 64  # 潜在空间维度\n",
    "    cond_dim = 4  # 季节条件向量维度\n",
    "    \n",
    "    # 模型架构\n",
    "    hidden_dim = 128\n",
    "    num_layers = 3\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # 训练参数\n",
    "    batch_size = 256\n",
    "    epochs_embed = 2\n",
    "    epochs_supervise = 2\n",
    "    epochs_joint = 3\n",
    "    lr = 0.001\n",
    "    gamma = 0.1  # 判别器损失系数\n",
    "    \n",
    "    # 优化器\n",
    "    use_amp = True  # 自动混合精度\n",
    "    lr_scheduler = True\n",
    "    \n",
    "    # 路径设置\n",
    "    data_paths = {\n",
    "        \"spring\": \"spring_data.csv\",\n",
    "        \"summer\": \"summer_data.csv\",\n",
    "        \"autumn\": \"autumn_data.csv\",\n",
    "        \"winter\": \"winter_data.csv\"\n",
    "    }\n",
    "    output_dir = \"./timegan_results/\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b374b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 模型组件 ======================\n",
    "class Embedder(nn.Module):\n",
    "    \"\"\" 将原始数据映射到潜在空间 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.input_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.latent_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 返回整个序列的嵌入\n",
    "        rnn_out, _ = self.rnn(x)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 对每个时间步应用全连接层\n",
    "        batch_size, seq_len, hidden_dim = rnn_out.shape\n",
    "        rnn_out_flat = rnn_out.reshape(-1, hidden_dim)  # [batch_size*seq_len, hidden_dim]\n",
    "        latent_flat = self.fc(rnn_out_flat)  # [batch_size*seq_len, latent_dim]\n",
    "        latent = latent_flat.view(batch_size, seq_len, -1)  # [batch_size, seq_len, latent_dim]\n",
    "        \n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6d9aec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recovery(nn.Module):\n",
    "    \"\"\" 将潜在表示映射回原始空间 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.latent_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.input_dim),\n",
    "            nn.Sigmoid()  # 如果数据归一化到[0,1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, h):\n",
    "        #h_expanded = self.fc(h).unsqueeze(1).repeat(1, Config.seq_len, 1)\n",
    "        output, _ = self.rnn(h)\n",
    "        # 对每个时间步应用全连接层\n",
    "        batch_size, seq_len, hidden_dim = output.shape\n",
    "        output_flat = output.reshape(-1, hidden_dim)\n",
    "        recovered_flat = self.fc(output_flat)\n",
    "        recovered = recovered_flat.view(batch_size, seq_len, -1)\n",
    "        return recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98bd0699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianGenerator(nn.Module):\n",
    "    \"\"\" 贝叶斯生成器: 从噪声生成潜在表示 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # 条件向量处理层\n",
    "        self.cond_fc = nn.Linear(config.cond_dim, config.noise_dim)\n",
    "        \n",
    "        # RNN主干网络\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.noise_dim * 2,  # 噪声 + 条件\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        \n",
    "        # 输出层 (均值和log方差)\n",
    "        self.fc_mu = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "    \n",
    "    def forward(self, z, cond):\n",
    "        # CUDA断言: 维度检查\n",
    "        assert z.dim() == 3, f\"z must be 3D tensor, got {z.dim()}D\"\n",
    "        assert cond.dim() == 2, f\"cond must be 2D tensor, got {cond.dim()}D\"\n",
    "        \n",
    "        # 条件向量处理\n",
    "        cond_proj = self.cond_fc(cond)\n",
    "        cond_expanded = cond_proj.unsqueeze(1).repeat(1, z.size(1), 1)\n",
    "        \n",
    "        # 拼接噪声和条件\n",
    "        z_cond = torch.cat([z, cond_expanded], dim=-1)\n",
    "        \n",
    "        # RNN处理\n",
    "        rnn_out, _ = self.rnn(z_cond)\n",
    "        \n",
    "        # 计算均值和方差\n",
    "        mu = self.fc_mu(rnn_out)\n",
    "        logvar = self.fc_logvar(rnn_out)\n",
    "        \n",
    "        # 重参数采样\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_hat = mu + eps * std\n",
    "        \n",
    "        return x_hat, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "39dbe68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" 判别器: 区分真实/生成的潜在表示 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.latent_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(config.hidden_dim, 1)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def forward(self, h):\n",
    "        output, _ = self.rnn(h)\n",
    "        return self.fc(output[:, -1, :])  # 取最后时间步输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ce44f198",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervisor(nn.Module):\n",
    "    \"\"\" 监督器: 学习时间序列的动态特性 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.latent_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers-1,  # 比生成器少一层\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        output, _ = self.rnn(h)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1df413e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 完整TimeGAN模型 ======================\n",
    "class TimeGAN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # 模块初始化\n",
    "        self.embedder = Embedder(config).to(config.device)\n",
    "        self.recovery = Recovery(config).to(config.device)\n",
    "        self.generator = BayesianGenerator(config).to(config.device)\n",
    "        self.discriminator = Discriminator(config).to(config.device)\n",
    "        self.supervisor = Supervisor(config).to(config.device)\n",
    "        \n",
    "        # 优化器\n",
    "        self.opt_embed = optim.Adam(\n",
    "            list(self.embedder.parameters()) + list(self.recovery.parameters()),\n",
    "            lr=config.lr\n",
    "        )\n",
    "        self.opt_supervise = optim.Adam(\n",
    "            self.supervisor.parameters(),\n",
    "            lr=config.lr\n",
    "        )\n",
    "        self.opt_joint = optim.Adam(\n",
    "            list(self.generator.parameters()) + \n",
    "            list(self.discriminator.parameters()) +\n",
    "            list(self.embedder.parameters()) +\n",
    "            list(self.recovery.parameters()) +\n",
    "            list(self.supervisor.parameters()),\n",
    "            lr=config.lr\n",
    "        )\n",
    "        \n",
    "        # 学习率调度器\n",
    "        if config.lr_scheduler:\n",
    "            self.scheduler_embed = optim.lr_scheduler.StepLR(self.opt_embed, step_size=50, gamma=0.5)\n",
    "            self.scheduler_supervise = optim.lr_scheduler.StepLR(self.opt_supervise, step_size=50, gamma=0.5)\n",
    "            self.scheduler_joint = optim.lr_scheduler.StepLR(self.opt_joint, step_size=100, gamma=0.5)\n",
    "        \n",
    "        # 损失函数\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        \n",
    "        # 混合精度训练\n",
    "        self.scaler = GradScaler(enabled=config.use_amp)\n",
    "        \n",
    "    def compute_kl_loss(self, mu, logvar):\n",
    "        \"\"\" 计算KL散度损失 \"\"\"\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / (mu.size(0) * mu.size(1))\n",
    "    \n",
    "    def compute_moment_loss(self, real, fake):\n",
    "        \"\"\" 统计矩匹配损失 \"\"\"\n",
    "        real_mean, real_std = real.mean(), real.std()\n",
    "        fake_mean, fake_std = fake.mean(), fake.std()\n",
    "        loss_v1 = self.mse_loss(real_mean, fake_mean)\n",
    "        loss_v2 = self.mse_loss(real_std, fake_std)\n",
    "        return loss_v1 + loss_v2\n",
    "    \n",
    "    def forward(self, x, cond, mode=\"train\"):\n",
    "        # 嵌入阶段\n",
    "        h = self.embedder(x)\n",
    "        x_tilde = self.recovery(h)\n",
    "        \n",
    "        # 监督阶段\n",
    "        h_hat_supervise = self.supervisor(h[:, :-1, :])\n",
    "        \n",
    "        # 生成阶段\n",
    "        z = torch.randn(x.size(0), self.config.seq_len, self.config.noise_dim).to(self.config.device)\n",
    "        e_hat, mu, logvar = self.generator(z, cond)\n",
    "        \n",
    "        # 判别阶段\n",
    "        y_real = self.discriminator(h)\n",
    "        y_fake = self.discriminator(e_hat)\n",
    "        y_fake_e = self.discriminator(e_hat.detach())\n",
    "        \n",
    "        return {\n",
    "            \"x_tilde\": x_tilde,\n",
    "            \"h\": h,\n",
    "            \"h_hat_supervise\": h_hat_supervise,\n",
    "            \"e_hat\": e_hat,\n",
    "            \"y_real\": y_real,\n",
    "            \"y_fake\": y_fake,\n",
    "            \"y_fake_e\": y_fake_e,\n",
    "            \"mu\": mu,\n",
    "            \"logvar\": logvar\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18c6dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 数据处理工具 ======================\n",
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.scalers = {}\n",
    "        self.cond_vectors = {\n",
    "            \"spring\": [1, 0, 0, 0],\n",
    "            \"summer\": [0, 1, 0, 0],\n",
    "            \"autumn\": [0, 0, 1, 0],\n",
    "            \"winter\": [0, 0, 0, 1]\n",
    "        }\n",
    "    \n",
    "    def load_and_process(self):\n",
    "        all_data = []\n",
    "        all_conditions = []\n",
    "        \n",
    "        for season, path in self.config.data_paths.items():\n",
    "            # 加载数据\n",
    "            df = pd.read_csv(path)\n",
    "            data = df.values\n",
    "            \n",
    "            # 归一化\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_data = scaler.fit_transform(data)\n",
    "            self.scalers[season] = scaler\n",
    "            \n",
    "            # 创建序列\n",
    "            num_samples = scaled_data.shape[0] - self.config.seq_len + 1\n",
    "            sequences = np.zeros((num_samples, self.config.seq_len, self.config.input_dim))\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                sequences[i] = scaled_data[i:i+self.config.seq_len]\n",
    "            \n",
    "            # 创建条件向量\n",
    "            conditions = np.tile(self.cond_vectors[season], (num_samples, 1))\n",
    "            \n",
    "            all_data.append(sequences)\n",
    "            all_conditions.append(conditions)\n",
    "        X= np.concatenate(all_data, axis=0)\n",
    "        C = np.concatenate(all_conditions, axis=0)\n",
    "        if np.isnan(X).any() or np.isinf(X).any():\n",
    "            print(\"警告: 输入数据包含NaN或Inf值!\")\n",
    "            X = np.nan_to_num(X, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    \n",
    "        if np.isnan(C).any() or np.isinf(C).any():\n",
    "            print(\"警告: 条件数据包含NaN或Inf值!\")\n",
    "            C = np.nan_to_num(C, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        # 合并所有季节数据\n",
    "        # X= np.concatenate(all_data, axis=0)\n",
    "        #C = np.concatenate(all_conditions, axis=0)\n",
    "        \n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(C, dtype=torch.float32)\n",
    "    \n",
    "    def inverse_transform(self, data, season):\n",
    "        \"\"\" 反归一化数据 \"\"\"\n",
    "        scaler = self.scalers[season]\n",
    "        return scaler.inverse_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 训练循环 ======================\n",
    "def train_embedding(model, dataloader, config):\n",
    "    \"\"\" 嵌入训练阶段 \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(config.epochs_embed):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for x, c in dataloader:\n",
    "            x = x.to(config.device)\n",
    "            \n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # 前向传播\n",
    "                h = model.embedder(x)\n",
    "                x_tilde = model.recovery(h)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = model.mse_loss(x_tilde, x)\n",
    "            \n",
    "            # 反向传播\n",
    "            model.opt_embed.zero_grad()\n",
    "            model.scaler.scale(loss).backward()\n",
    "            model.scaler.step(model.opt_embed)\n",
    "            model.scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if config.lr_scheduler:\n",
    "            model.scheduler_embed.step()\n",
    "        \n",
    "        print(f\"Embedding Epoch [{epoch+1}/{config.epochs_embed}] Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def train_supervisor(model, dataloader, config):\n",
    "    \"\"\" 监督训练阶段 \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(config.epochs_supervise):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for x, c in dataloader:\n",
    "            x = x.to(config.device)\n",
    "            \n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # 前向传播\n",
    "                h = model.embedder(x)\n",
    "                # 监督器输入: 从第一个到倒数第二个时间步\n",
    "                # 监督器输出: 预测下一个时间步\n",
    "                h_hat_supervise = model.supervisor(h[:, :-1, :])  # 输入: [batch, seq_len-1, latent_dim]\n",
    "                # 目标: 从第二个时间步开始到最后一个\n",
    "                target = h[:, 1:, :]  # [batch, seq_len-1, latent_dim]\n",
    "\n",
    "                # 监督损失 - 确保维度匹配\n",
    "                loss = model.mse_loss(h_hat_supervise, target)\n",
    "            # 反向传播\n",
    "            model.opt_supervise.zero_grad()\n",
    "            model.scaler.scale(loss).backward()\n",
    "            model.scaler.step(model.opt_supervise)\n",
    "            model.scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if config.lr_scheduler:\n",
    "            model.scheduler_supervise.step()\n",
    "        \n",
    "        print(f\"Supervisor Epoch [{epoch+1}/{config.epochs_supervise}] Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def train_joint(model, dataloader, config):\n",
    "    \"\"\" 联合训练阶段 \"\"\"\n",
    "    model.train()\n",
    "    history = {\n",
    "        \"G_loss\": [], \"D_loss\": [], \"E_loss\": [],\n",
    "        \"G_adv\": [], \"G_supervise\": [], \"G_moment\": [], \"G_kl\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(config.epochs_joint):\n",
    "        epoch_losses = {k: 0.0 for k in history.keys()}\n",
    "        \n",
    "        for x, c in dataloader:\n",
    "            x, c = x.to(config.device), c.to(config.device)\n",
    "            \n",
    "            # ===== 判别器训练 =====\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # 前向传播\n",
    "                outputs = model(x, c)\n",
    "                \n",
    "                # 判别器损失\n",
    "                D_loss_real = model.bce_loss(outputs[\"y_real\"], torch.ones_like(outputs[\"y_real\"]))\n",
    "                D_loss_fake = model.bce_loss(outputs[\"y_fake\"], torch.zeros_like(outputs[\"y_fake\"]))\n",
    "                D_loss_fake_e = model.bce_loss(outputs[\"y_fake_e\"], torch.zeros_like(outputs[\"y_fake_e\"]))\n",
    "                D_loss = D_loss_real + D_loss_fake + config.gamma * D_loss_fake_e\n",
    "            \n",
    "            model.opt_joint.zero_grad()\n",
    "            model.scaler.scale(D_loss).backward(retain_graph=True)\n",
    "            model.scaler.step(model.opt_joint)\n",
    "            \n",
    "            # ===== 生成器训练 =====\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # 对抗损失\n",
    "                G_loss_U = model.bce_loss(outputs[\"y_fake\"], torch.ones_like(outputs[\"y_fake\"]))\n",
    "                G_loss_U_e = model.bce_loss(outputs[\"y_fake_e\"], torch.ones_like(outputs[\"y_fake_e\"]))\n",
    "                \n",
    "                # 监督损失\n",
    "                G_loss_S = model.mse_loss(outputs[\"h\"][:, 1:, :], outputs[\"h_hat_supervise\"])\n",
    "\n",
    "                \n",
    "                # 统计矩匹配损失\n",
    "                G_loss_V = model.compute_moment_loss(outputs[\"h\"], outputs[\"e_hat\"])\n",
    "                \n",
    "                # KL散度损失\n",
    "                G_KL_loss = model.compute_kl_loss(outputs[\"mu\"], outputs[\"logvar\"])\n",
    "                \n",
    "                # 嵌入和恢复损失\n",
    "                E_loss_T0 = model.mse_loss(outputs[\"x_tilde\"], x)\n",
    "                E_loss0 = model.mse_loss(outputs[\"h\"], model.embedder(x))\n",
    "                E_loss = E_loss0 + 0.1 * G_loss_S\n",
    "                \n",
    "                # 总损失\n",
    "                G_loss = G_loss_U + G_loss_U_e + 100 * torch.sqrt(G_loss_S) + 100 * G_loss_V + G_KL_loss + E_loss\n",
    "            \n",
    "            model.opt_joint.zero_grad()\n",
    "            model.scaler.scale(G_loss).backward()\n",
    "            model.scaler.step(model.opt_joint)\n",
    "            model.scaler.update()\n",
    "            \n",
    "            # 记录损失\n",
    "            losses = {\n",
    "                \"G_loss\": G_loss.item(),\n",
    "                \"D_loss\": D_loss.item(),\n",
    "                \"E_loss\": E_loss.item(),\n",
    "                \"G_adv\": (G_loss_U + G_loss_U_e).item(),\n",
    "                \"G_supervise\": G_loss_S.item(),\n",
    "                \"G_moment\": G_loss_V.item(),\n",
    "                \"G_kl\": G_KL_loss.item()\n",
    "            }\n",
    "            \n",
    "            for k in epoch_losses:\n",
    "                epoch_losses[k] += losses[k]\n",
    "        \n",
    "        # 计算平均损失\n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] /= len(dataloader)\n",
    "            history[k].append(epoch_losses[k])\n",
    "        \n",
    "        if config.lr_scheduler:\n",
    "            model.scheduler_joint.step()\n",
    "        \n",
    "        print(f\"Joint Epoch [{epoch+1}/{config.epochs_joint}] G_loss: {epoch_losses['G_loss']:.6f} D_loss: {epoch_losses['D_loss']:.6f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "953e8c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(embed_loss, super_loss, joint_history, config):\n",
    "    \"\"\" 绘制损失曲线 \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 14,\n",
    "        'axes.titlesize': 18,\n",
    "        'axes.labelsize': 16,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'figure.figsize': (15, 10),\n",
    "        'figure.dpi': 300,\n",
    "        'savefig.dpi': 300,\n",
    "        'lines.linewidth': 2.5,\n",
    "        'legend.fontsize': 14\n",
    "    })\n",
    "    colors = sns.color_palette(\"husl\", 8)\n",
    "    \n",
    "    # 嵌入损失\n",
    "    plt.figure()\n",
    "    plt.plot(embed_loss, color=colors[0])\n",
    "    plt.title(\"Embedding Training Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"embed_loss.svg\"))\n",
    "    \n",
    "    # 监督损失\n",
    "    plt.figure()\n",
    "    plt.plot(super_loss, color=colors[1])\n",
    "    plt.title(\"Supervisor Training Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"supervise_loss.svg\"))\n",
    "    \n",
    "    # 联合训练损失\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 总损失\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(joint_history[\"G_loss\"], label=\"Generator Loss\", color=colors[0])\n",
    "    plt.plot(joint_history[\"D_loss\"], label=\"Discriminator Loss\", color=colors[1])\n",
    "    plt.plot(joint_history[\"E_loss\"], label=\"Embedding Loss\", color=colors[2])\n",
    "    plt.title(\"Total Losses\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 生成器损失分解\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(joint_history[\"G_adv\"], label=\"Adversarial Loss\", color=colors[0])\n",
    "    plt.plot(joint_history[\"G_supervise\"], label=\"Supervise Loss\", color=colors[1])\n",
    "    plt.plot(joint_history[\"G_moment\"], label=\"Moment Loss\", color=colors[2])\n",
    "    plt.plot(joint_history[\"G_kl\"], label=\"KL Loss\", color=colors[3])\n",
    "    plt.title(\"Generator Loss Components\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 对抗损失细节\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(joint_history[\"G_adv\"], label=\"Generator Adv Loss\", color=colors[0])\n",
    "    plt.plot(joint_history[\"D_loss\"], label=\"Discriminator Loss\", color=colors[1])\n",
    "    plt.title(\"Adversarial Losses\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # KL和矩损失\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(joint_history[\"G_kl\"], label=\"KL Divergence\", color=colors[3])\n",
    "    plt.plot(joint_history[\"G_moment\"], label=\"Moment Matching\", color=colors[2])\n",
    "    plt.title(\"Bayesian and Statistical Losses\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"joint_training_losses.svg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bfa29725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "警告: 输入数据包含NaN或Inf值!\n",
      "Starting Embedding Training...\n",
      "Embedding Epoch [1/2] Loss: 0.057101\n",
      "Embedding Epoch [2/2] Loss: 0.017625\n",
      "\n",
      "Starting Supervisor Training...\n",
      "Supervisor Epoch [1/2] Loss: 0.060660\n",
      "Supervisor Epoch [2/2] Loss: 0.006187\n",
      "\n",
      "Starting Joint Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (167) must match the size of tensor b (166) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# CUDA断言错误检查\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[36], line 25\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m super_loss \u001b[38;5;241m=\u001b[39m train_supervisor(model, dataloader, config)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Joint Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m joint_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_joint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 可视化损失\u001b[39;00m\n\u001b[0;32m     28\u001b[0m plot_losses(embed_loss, super_loss, joint_history, config)\n",
      "Cell \u001b[1;32mIn[34], line 115\u001b[0m, in \u001b[0;36mtrain_joint\u001b[1;34m(model, dataloader, config)\u001b[0m\n\u001b[0;32m    112\u001b[0m G_loss_U_e \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbce_loss(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_fake_e\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39mones_like(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_fake_e\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# 监督损失\u001b[39;00m\n\u001b[1;32m--> 115\u001b[0m G_loss_S \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh_hat_supervise\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# 统计矩匹配损失\u001b[39;00m\n\u001b[0;32m    118\u001b[0m G_loss_V \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_moment_loss(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m\"\u001b[39m], outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124me_hat\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:3365\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3363\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3365\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (167) must match the size of tensor b (166) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # 在main函数开头添加\n",
    "def main():\n",
    "    # 初始化配置\n",
    "    config = Config()\n",
    "    print(f\"Using device: {config.device}\")\n",
    "    \n",
    "    # 数据处理\n",
    "    processor = DataProcessor(config)\n",
    "    X, C = processor.load_and_process()\n",
    "    dataset = TensorDataset(X, C)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = TimeGAN(config)\n",
    "    \n",
    "    # 训练阶段\n",
    "    print(\"Starting Embedding Training...\")\n",
    "    embed_loss = train_embedding(model, dataloader, config)\n",
    "    \n",
    "    print(\"\\nStarting Supervisor Training...\")\n",
    "    super_loss = train_supervisor(model, dataloader, config)\n",
    "    \n",
    "    print(\"\\nStarting Joint Training...\")\n",
    "    joint_history = train_joint(model, dataloader, config)\n",
    "    \n",
    "    # 可视化损失\n",
    "    plot_losses(embed_loss, super_loss, joint_history, config)\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save({\n",
    "        'embedder': model.embedder.state_dict(),\n",
    "        'recovery': model.recovery.state_dict(),\n",
    "        'generator': model.generator.state_dict(),\n",
    "        'discriminator': model.discriminator.state_dict(),\n",
    "        'supervisor': model.supervisor.state_dict(),\n",
    "        'config': config\n",
    "    }, os.path.join(config.output_dir, \"timegan_model.pth\"))\n",
    "    \n",
    "    print(\"Training completed and model saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CUDA断言错误检查\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb92e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fc8707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5f3ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f544bad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379b240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08814b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a002cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2aff1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2321c42f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
