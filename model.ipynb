{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "942d913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fa87655e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 配置类 ======================\n",
    "class Config:\n",
    "    # 设备配置\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 数据参数\n",
    "    seq_len = 24 * 7  # 一周数据 (小时级)\n",
    "    input_dim = 12  # 特征维度: wind, solar, electric, cooling, heating, temperature, day_cos, day_sin, hour_cos, hour_sin, mean, std\n",
    "    noise_dim = 32  # 噪声维度\n",
    "    latent_dim = 64  # 潜在空间维度\n",
    "    cond_dim = 4  # 季节条件向量维度\n",
    "    \n",
    "    # 模型架构\n",
    "    hidden_dim = 128\n",
    "    num_layers = 3\n",
    "    dropout = 0.1\n",
    "    \n",
    "    # 训练参数\n",
    "    batch_size = 256\n",
    "    epochs_embed = 10\n",
    "    epochs_supervise = 20\n",
    "    epochs_joint = 30\n",
    "    lr = 0.001\n",
    "    gamma = 0.1  # 判别器损失系数\n",
    "    \n",
    "    # 优化器\n",
    "    use_amp = True  # 自动混合精度\n",
    "    lr_scheduler = True\n",
    "    \n",
    "    # 路径设置\n",
    "    data_paths = {\n",
    "        \"spring\": \"spring_data.csv\",\n",
    "        \"summer\": \"summer_data.csv\",\n",
    "        \"autumn\": \"autumn_data.csv\",\n",
    "        \"winter\": \"winter_data.csv\"\n",
    "    }\n",
    "    output_dir = \"./timegan_results/\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        os.makedirs(self.output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "261311b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 模型组件 ======================\n",
    "class Embedder(nn.Module):\n",
    "    \"\"\" 将原始数据映射到潜在空间 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.input_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.latent_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 返回整个序列的嵌入\n",
    "        rnn_out, _ = self.rnn(x)  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        # 对每个时间步应用全连接层\n",
    "        batch_size, seq_len, hidden_dim = rnn_out.shape\n",
    "        rnn_out_flat = rnn_out.reshape(-1, hidden_dim)  # [batch_size*seq_len, hidden_dim]\n",
    "        latent_flat = self.fc(rnn_out_flat)  # [batch_size*seq_len, latent_dim]\n",
    "        latent = latent_flat.view(batch_size, seq_len, -1)  # [batch_size, seq_len, latent_dim]\n",
    "        \n",
    "        return latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b34dd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Recovery(nn.Module):\n",
    "    \"\"\" 将潜在表示映射回原始空间 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.latent_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, config.input_dim),\n",
    "            nn.Sigmoid()  # 如果数据归一化到[0,1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, h):\n",
    "        #h_expanded = self.fc(h).unsqueeze(1).repeat(1, Config.seq_len, 1)\n",
    "        output, _ = self.rnn(h)\n",
    "        # 对每个时间步应用全连接层\n",
    "        batch_size, seq_len, hidden_dim = output.shape\n",
    "        output_flat = output.reshape(-1, hidden_dim)\n",
    "        recovered_flat = self.fc(output_flat)\n",
    "        recovered = recovered_flat.view(batch_size, seq_len, -1)\n",
    "        return recovered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0644fce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianGenerator(nn.Module):\n",
    "    \"\"\" 贝叶斯生成器: 从噪声生成潜在表示 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # 条件向量处理层\n",
    "        self.cond_fc = nn.Linear(config.cond_dim, config.noise_dim)\n",
    "        \n",
    "        # RNN主干网络\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.noise_dim * 2,  # 噪声 + 条件\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        \n",
    "        # 输出层 (均值和log方差)\n",
    "        self.fc_mu = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "        self.fc_logvar = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "    \n",
    "    def forward(self, z, cond):\n",
    "        # CUDA断言: 维度检查\n",
    "        assert z.dim() == 3, f\"z must be 3D tensor, got {z.dim()}D\"\n",
    "        assert cond.dim() == 2, f\"cond must be 2D tensor, got {cond.dim()}D\"\n",
    "        \n",
    "        # 条件向量处理\n",
    "        cond_proj = self.cond_fc(cond)\n",
    "        cond_expanded = cond_proj.unsqueeze(1).repeat(1, z.size(1), 1)\n",
    "        \n",
    "        # 拼接噪声和条件\n",
    "        z_cond = torch.cat([z, cond_expanded], dim=-1)\n",
    "        \n",
    "        # RNN处理\n",
    "        rnn_out, _ = self.rnn(z_cond)\n",
    "        \n",
    "        # 计算均值和方差\n",
    "        mu = self.fc_mu(rnn_out)\n",
    "        logvar = self.fc_logvar(rnn_out)\n",
    "        \n",
    "        # 重参数采样\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_hat = mu + eps * std\n",
    "        \n",
    "        return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "82481669",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\" 判别器: 区分真实/生成的潜在表示 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.latent_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(config.hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, h):\n",
    "        output, _ = self.rnn(h)\n",
    "        return self.fc(output[:, -1, :])  # 取最后时间步输出\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a39dda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Supervisor(nn.Module):\n",
    "    \"\"\" 监督器: 学习时间序列的动态特性 \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(\n",
    "            input_size=config.latent_dim,\n",
    "            hidden_size=config.hidden_dim,\n",
    "            num_layers=config.num_layers-1,  # 比生成器少一层\n",
    "            batch_first=True,\n",
    "            dropout=config.dropout\n",
    "        )\n",
    "        self.fc = nn.Linear(config.hidden_dim, config.latent_dim)\n",
    "    \n",
    "    def forward(self, h):\n",
    "        output, _ = self.rnn(h)\n",
    "        return self.fc(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6b9e3481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 完整TimeGAN模型 ======================\n",
    "class TimeGAN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # 模块初始化\n",
    "        self.embedder = Embedder(config).to(config.device)\n",
    "        self.recovery = Recovery(config).to(config.device)\n",
    "        self.generator = BayesianGenerator(config).to(config.device)\n",
    "        self.discriminator = Discriminator(config).to(config.device)\n",
    "        self.supervisor = Supervisor(config).to(config.device)\n",
    "        \n",
    "        # 优化器\n",
    "        self.opt_embed = optim.Adam(\n",
    "            list(self.embedder.parameters()) + list(self.recovery.parameters()),\n",
    "            lr=config.lr\n",
    "        )\n",
    "        self.opt_supervise = optim.Adam(\n",
    "            self.supervisor.parameters(),\n",
    "            lr=config.lr\n",
    "        )\n",
    "        self.opt_joint = optim.Adam(\n",
    "            list(self.generator.parameters()) + \n",
    "            list(self.discriminator.parameters()) +\n",
    "            list(self.embedder.parameters()) +\n",
    "            list(self.recovery.parameters()) +\n",
    "            list(self.supervisor.parameters()),\n",
    "            lr=config.lr\n",
    "        )\n",
    "        \n",
    "        # 学习率调度器\n",
    "        if config.lr_scheduler:\n",
    "            self.scheduler_embed = optim.lr_scheduler.StepLR(self.opt_embed, step_size=50, gamma=0.5)\n",
    "            self.scheduler_supervise = optim.lr_scheduler.StepLR(self.opt_supervise, step_size=50, gamma=0.5)\n",
    "            self.scheduler_joint = optim.lr_scheduler.StepLR(self.opt_joint, step_size=100, gamma=0.5)\n",
    "        \n",
    "        # 损失函数\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        \n",
    "        # 混合精度训练\n",
    "        self.scaler = GradScaler(enabled=config.use_amp)\n",
    "        \n",
    "    def compute_kl_loss(self, mu, logvar):\n",
    "        \"\"\" 计算KL散度损失 \"\"\"\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / (mu.size(0) * mu.size(1))\n",
    "    \n",
    "    def compute_moment_loss(self, real, fake):\n",
    "        \"\"\" 统计矩匹配损失 \"\"\"\n",
    "        real_mean, real_std = real.mean(), real.std()\n",
    "        fake_mean, fake_std = fake.mean(), fake.std()\n",
    "        loss_v1 = self.mse_loss(real_mean, fake_mean)\n",
    "        loss_v2 = self.mse_loss(real_std, fake_std)\n",
    "        return loss_v1 + loss_v2\n",
    "    \n",
    "    def forward(self, x, cond, mode=\"train\"):\n",
    "        # 嵌入阶段\n",
    "        h = self.embedder(x)\n",
    "        x_tilde = self.recovery(h)\n",
    "        \n",
    "        # 监督阶段\n",
    "        h_hat_supervise = self.supervisor(h[:, :-1, :])\n",
    "        \n",
    "        # 生成阶段\n",
    "        z = torch.randn(x.size(0), self.config.seq_len, self.config.noise_dim).to(self.config.device)\n",
    "        e_hat, mu, logvar = self.generator(z, cond)\n",
    "        \n",
    "        # 判别阶段\n",
    "        y_real = self.discriminator(h)\n",
    "        y_fake = self.discriminator(e_hat)\n",
    "        y_fake_e = self.discriminator(e_hat.detach())\n",
    "        \n",
    "        return {\n",
    "            \"x_tilde\": x_tilde,\n",
    "            \"h\": h,\n",
    "            \"h_hat_supervise\": h_hat_supervise,\n",
    "            \"e_hat\": e_hat,\n",
    "            \"y_real\": y_real,\n",
    "            \"y_fake\": y_fake,\n",
    "            \"y_fake_e\": y_fake_e,\n",
    "            \"mu\": mu,\n",
    "            \"logvar\": logvar\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c74944e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 数据处理工具 ======================\n",
    "class DataProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.scalers = {}\n",
    "        self.cond_vectors = {\n",
    "            \"spring\": [1, 0, 0, 0],\n",
    "            \"summer\": [0, 1, 0, 0],\n",
    "            \"autumn\": [0, 0, 1, 0],\n",
    "            \"winter\": [0, 0, 0, 1]\n",
    "        }\n",
    "    \n",
    "    def load_and_process(self):\n",
    "        all_data = []\n",
    "        all_conditions = []\n",
    "        \n",
    "        for season, path in self.config.data_paths.items():\n",
    "            # 加载数据\n",
    "            df = pd.read_csv(path)\n",
    "            data = df.values\n",
    "            \n",
    "            # 归一化\n",
    "            scaler = MinMaxScaler()\n",
    "            scaled_data = scaler.fit_transform(data)\n",
    "            self.scalers[season] = scaler\n",
    "            \n",
    "            # 创建序列\n",
    "            num_samples = scaled_data.shape[0] - self.config.seq_len + 1\n",
    "            sequences = np.zeros((num_samples, self.config.seq_len, self.config.input_dim))\n",
    "            \n",
    "            for i in range(num_samples):\n",
    "                sequences[i] = scaled_data[i:i+self.config.seq_len]\n",
    "            \n",
    "            # 创建条件向量\n",
    "            conditions = np.tile(self.cond_vectors[season], (num_samples, 1))\n",
    "            \n",
    "            all_data.append(sequences)\n",
    "            all_conditions.append(conditions)\n",
    "        X= np.concatenate(all_data, axis=0)\n",
    "        C = np.concatenate(all_conditions, axis=0)\n",
    "        if np.isnan(X).any() or np.isinf(X).any():\n",
    "            print(\"警告: 输入数据包含NaN或Inf值!\")\n",
    "            X = np.nan_to_num(X, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "    \n",
    "        if np.isnan(C).any() or np.isinf(C).any():\n",
    "            print(\"警告: 条件数据包含NaN或Inf值!\")\n",
    "            C = np.nan_to_num(C, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "        # 合并所有季节数据\n",
    "        # X= np.concatenate(all_data, axis=0)\n",
    "        #C = np.concatenate(all_conditions, axis=0)\n",
    "        \n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(C, dtype=torch.float32)\n",
    "    \n",
    "    def inverse_transform(self, data, season):\n",
    "        \"\"\" 反归一化数据 \"\"\"\n",
    "        scaler = self.scalers[season]\n",
    "        return scaler.inverse_transform(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "53248e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== 训练循环 ======================\n",
    "def train_embedding(model, dataloader, config):\n",
    "    \"\"\" 嵌入训练阶段 \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(config.epochs_embed):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for x, c in dataloader:\n",
    "            x = x.to(config.device)\n",
    "            \n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # 前向传播\n",
    "                h = model.embedder(x)\n",
    "                x_tilde = model.recovery(h)\n",
    "                \n",
    "                # 计算损失\n",
    "                loss = model.mse_loss(x_tilde, x)\n",
    "            \n",
    "            # 反向传播\n",
    "            model.opt_embed.zero_grad()\n",
    "            model.scaler.scale(loss).backward()\n",
    "            model.scaler.step(model.opt_embed)\n",
    "            model.scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if config.lr_scheduler:\n",
    "            model.scheduler_embed.step()\n",
    "        \n",
    "        print(f\"Embedding Epoch [{epoch+1}/{config.epochs_embed}] Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def train_supervisor(model, dataloader, config):\n",
    "    \"\"\" 监督训练阶段 \"\"\"\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    for epoch in range(config.epochs_supervise):\n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for x, c in dataloader:\n",
    "            x = x.to(config.device)\n",
    "            \n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # 前向传播\n",
    "                h = model.embedder(x)\n",
    "                # 监督器输入: 从第一个到倒数第二个时间步\n",
    "                # 监督器输出: 预测下一个时间步\n",
    "                h_hat_supervise = model.supervisor(h[:, :-1, :])  # 输入: [batch, seq_len-1, latent_dim]\n",
    "                # 目标: 从第二个时间步开始到最后一个\n",
    "                target = h[:, 1:, :]  # [batch, seq_len-1, latent_dim]\n",
    "\n",
    "                # 监督损失 - 确保维度匹配\n",
    "                loss = model.mse_loss(h_hat_supervise, target)\n",
    "            # 反向传播\n",
    "            model.opt_supervise.zero_grad()\n",
    "            model.scaler.scale(loss).backward()\n",
    "            model.scaler.step(model.opt_supervise)\n",
    "            model.scaler.update()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        if config.lr_scheduler:\n",
    "            model.scheduler_supervise.step()\n",
    "        \n",
    "        print(f\"Supervisor Epoch [{epoch+1}/{config.epochs_supervise}] Loss: {avg_loss:.6f}\")\n",
    "    \n",
    "    return losses\n",
    "\n",
    "def train_joint(model, dataloader, config):\n",
    "    \"\"\" 联合训练阶段 \"\"\"\n",
    "    model.train()\n",
    "    history = {\n",
    "        \"G_loss\": [], \"D_loss\": [], \"E_loss\": [],\n",
    "        \"G_adv\": [], \"G_supervise\": [], \"G_moment\": [], \"G_kl\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(config.epochs_joint):\n",
    "        epoch_losses = {k: 0.0 for k in history.keys()}\n",
    "        \n",
    "        for x, c in dataloader:\n",
    "            x, c = x.to(config.device), c.to(config.device)\n",
    "            \n",
    "            # ===== 判别器训练 =====\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # 前向传播\n",
    "                outputs = model(x, c)\n",
    "                \n",
    "                # 判别器损失\n",
    "                D_loss_real = model.bce_loss(outputs[\"y_real\"], torch.ones_like(outputs[\"y_real\"]))\n",
    "                D_loss_fake = model.bce_loss(outputs[\"y_fake\"], torch.zeros_like(outputs[\"y_fake\"]))\n",
    "                D_loss_fake_e = model.bce_loss(outputs[\"y_fake_e\"], torch.zeros_like(outputs[\"y_fake_e\"]))\n",
    "                D_loss = D_loss_real + D_loss_fake + config.gamma * D_loss_fake_e\n",
    "            \n",
    "            model.opt_joint.zero_grad()\n",
    "            model.scaler.scale(D_loss).backward(retain_graph=True)\n",
    "            model.scaler.step(model.opt_joint)\n",
    "            \n",
    "            # ===== 生成器训练 =====\n",
    "            with autocast(enabled=config.use_amp):\n",
    "                # 对抗损失\n",
    "                G_loss_U = model.bce_loss(outputs[\"y_fake\"], torch.ones_like(outputs[\"y_fake\"]))\n",
    "                G_loss_U_e = model.bce_loss(outputs[\"y_fake_e\"], torch.ones_like(outputs[\"y_fake_e\"]))\n",
    "                \n",
    "                # 监督损失\n",
    "                G_loss_S = model.mse_loss(outputs[\"h\"][:, 1:, :], outputs[\"h_hat_supervise\"][:, :-1, :])\n",
    "                \n",
    "                # 统计矩匹配损失\n",
    "                G_loss_V = model.compute_moment_loss(outputs[\"h\"], outputs[\"e_hat\"])\n",
    "                \n",
    "                # KL散度损失\n",
    "                G_KL_loss = model.compute_kl_loss(outputs[\"mu\"], outputs[\"logvar\"])\n",
    "                \n",
    "                # 嵌入和恢复损失\n",
    "                E_loss_T0 = model.mse_loss(outputs[\"x_tilde\"], x)\n",
    "                E_loss0 = model.mse_loss(outputs[\"h\"], model.embedder(x))\n",
    "                E_loss = E_loss0 + 0.1 * G_loss_S\n",
    "                \n",
    "                # 总损失\n",
    "                G_loss = G_loss_U + G_loss_U_e + 100 * torch.sqrt(G_loss_S) + 100 * G_loss_V + G_KL_loss + E_loss\n",
    "            \n",
    "            model.opt_joint.zero_grad()\n",
    "            model.scaler.scale(G_loss).backward()\n",
    "            model.scaler.step(model.opt_joint)\n",
    "            model.scaler.update()\n",
    "            \n",
    "            # 记录损失\n",
    "            losses = {\n",
    "                \"G_loss\": G_loss.item(),\n",
    "                \"D_loss\": D_loss.item(),\n",
    "                \"E_loss\": E_loss.item(),\n",
    "                \"G_adv\": (G_loss_U + G_loss_U_e).item(),\n",
    "                \"G_supervise\": G_loss_S.item(),\n",
    "                \"G_moment\": G_loss_V.item(),\n",
    "                \"G_kl\": G_KL_loss.item()\n",
    "            }\n",
    "            \n",
    "            for k in epoch_losses:\n",
    "                epoch_losses[k] += losses[k]\n",
    "        \n",
    "        # 计算平均损失\n",
    "        for k in epoch_losses:\n",
    "            epoch_losses[k] /= len(dataloader)\n",
    "            history[k].append(epoch_losses[k])\n",
    "        \n",
    "        if config.lr_scheduler:\n",
    "            model.scheduler_joint.step()\n",
    "        \n",
    "        print(f\"Joint Epoch [{epoch+1}/{config.epochs_joint}] G_loss: {epoch_losses['G_loss']:.6f} D_loss: {epoch_losses['D_loss']:.6f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "feceeb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(embed_loss, super_loss, joint_history, config):\n",
    "    \"\"\" 绘制损失曲线 \"\"\"\n",
    "    plt.style.use('seaborn-v0_8-paper')\n",
    "    plt.rcParams.update({\n",
    "        'font.size': 14,\n",
    "        'axes.titlesize': 18,\n",
    "        'axes.labelsize': 16,\n",
    "        'xtick.labelsize': 12,\n",
    "        'ytick.labelsize': 12,\n",
    "        'figure.figsize': (15, 10),\n",
    "        'figure.dpi': 300,\n",
    "        'savefig.dpi': 300,\n",
    "        'lines.linewidth': 2.5,\n",
    "        'legend.fontsize': 14\n",
    "    })\n",
    "    colors = sns.color_palette(\"husl\", 8)\n",
    "    \n",
    "    # 嵌入损失\n",
    "    plt.figure()\n",
    "    plt.plot(embed_loss, color=colors[0])\n",
    "    plt.title(\"Embedding Training Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"embed_loss.svg\"))\n",
    "    \n",
    "    # 监督损失\n",
    "    plt.figure()\n",
    "    plt.plot(super_loss, color=colors[1])\n",
    "    plt.title(\"Supervisor Training Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"MSE Loss\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"supervise_loss.svg\"))\n",
    "    \n",
    "    # 联合训练损失\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # 总损失\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(joint_history[\"G_loss\"], label=\"Generator Loss\", color=colors[0])\n",
    "    plt.plot(joint_history[\"D_loss\"], label=\"Discriminator Loss\", color=colors[1])\n",
    "    plt.plot(joint_history[\"E_loss\"], label=\"Embedding Loss\", color=colors[2])\n",
    "    plt.title(\"Total Losses\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 生成器损失分解\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(joint_history[\"G_adv\"], label=\"Adversarial Loss\", color=colors[0])\n",
    "    plt.plot(joint_history[\"G_supervise\"], label=\"Supervise Loss\", color=colors[1])\n",
    "    plt.plot(joint_history[\"G_moment\"], label=\"Moment Loss\", color=colors[2])\n",
    "    plt.plot(joint_history[\"G_kl\"], label=\"KL Loss\", color=colors[3])\n",
    "    plt.title(\"Generator Loss Components\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 对抗损失细节\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(joint_history[\"G_adv\"], label=\"Generator Adv Loss\", color=colors[0])\n",
    "    plt.plot(joint_history[\"D_loss\"], label=\"Discriminator Loss\", color=colors[1])\n",
    "    plt.title(\"Adversarial Losses\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # KL和矩损失\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(joint_history[\"G_kl\"], label=\"KL Divergence\", color=colors[3])\n",
    "    plt.plot(joint_history[\"G_moment\"], label=\"Moment Matching\", color=colors[2])\n",
    "    plt.title(\"Bayesian and Statistical Losses\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(config.output_dir, \"joint_training_losses.svg\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b5561876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "警告: 输入数据包含NaN或Inf值!\n",
      "Starting Embedding Training...\n",
      "Embedding Epoch [1/10] Loss: 0.057169\n",
      "Embedding Epoch [2/10] Loss: 0.017803\n",
      "Embedding Epoch [3/10] Loss: 0.011160\n",
      "Embedding Epoch [4/10] Loss: 0.009143\n",
      "Embedding Epoch [5/10] Loss: 0.008179\n",
      "Embedding Epoch [6/10] Loss: 0.007541\n",
      "Embedding Epoch [7/10] Loss: 0.006976\n",
      "Embedding Epoch [8/10] Loss: 0.006382\n",
      "Embedding Epoch [9/10] Loss: 0.005670\n",
      "Embedding Epoch [10/10] Loss: 0.004952\n",
      "\n",
      "Starting Supervisor Training...\n",
      "Supervisor Epoch [1/20] Loss: 0.098014\n",
      "Supervisor Epoch [2/20] Loss: 0.016663\n",
      "Supervisor Epoch [3/20] Loss: 0.012459\n",
      "Supervisor Epoch [4/20] Loss: 0.010685\n",
      "Supervisor Epoch [5/20] Loss: 0.009537\n",
      "Supervisor Epoch [6/20] Loss: 0.008701\n",
      "Supervisor Epoch [7/20] Loss: 0.008045\n",
      "Supervisor Epoch [8/20] Loss: 0.007529\n",
      "Supervisor Epoch [9/20] Loss: 0.007119\n",
      "Supervisor Epoch [10/20] Loss: 0.006774\n",
      "Supervisor Epoch [11/20] Loss: 0.006489\n",
      "Supervisor Epoch [12/20] Loss: 0.006244\n",
      "Supervisor Epoch [13/20] Loss: 0.006036\n",
      "Supervisor Epoch [14/20] Loss: 0.005851\n",
      "Supervisor Epoch [15/20] Loss: 0.005687\n",
      "Supervisor Epoch [16/20] Loss: 0.005536\n",
      "Supervisor Epoch [17/20] Loss: 0.005411\n",
      "Supervisor Epoch [18/20] Loss: 0.005292\n",
      "Supervisor Epoch [19/20] Loss: 0.005186\n",
      "Supervisor Epoch [20/20] Loss: 0.005096\n",
      "\n",
      "Starting Joint Training...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\nor torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\nsafe to autocast.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# CUDA断言错误检查\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 25\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m super_loss \u001b[38;5;241m=\u001b[39m train_supervisor(model, dataloader, config)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting Joint Training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m joint_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_joint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 可视化损失\u001b[39;00m\n\u001b[0;32m     28\u001b[0m plot_losses(embed_loss, super_loss, joint_history, config)\n",
      "Cell \u001b[1;32mIn[78], line 99\u001b[0m, in \u001b[0;36mtrain_joint\u001b[1;34m(model, dataloader, config)\u001b[0m\n\u001b[0;32m     96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(x, c)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# 判别器损失\u001b[39;00m\n\u001b[1;32m---> 99\u001b[0m D_loss_real \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbce_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_real\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my_real\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m D_loss_fake \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbce_loss(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_fake\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39mzeros_like(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_fake\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n\u001b[0;32m    101\u001b[0m D_loss_fake_e \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbce_loss(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_fake_e\u001b[39m\u001b[38;5;124m\"\u001b[39m], torch\u001b[38;5;241m.\u001b[39mzeros_like(outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_fake_e\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\loss.py:618\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 618\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\app\\Anaconda\\envs\\torch\\lib\\site-packages\\torch\\nn\\functional.py:3154\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3151\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m   3152\u001b[0m     weight \u001b[38;5;241m=\u001b[39m weight\u001b[38;5;241m.\u001b[39mexpand(new_size)\n\u001b[1;32m-> 3154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction_enum\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.nn.functional.binary_cross_entropy and torch.nn.BCELoss are unsafe to autocast.\nMany models use a sigmoid layer right before the binary cross entropy layer.\nIn this case, combine the two layers using torch.nn.functional.binary_cross_entropy_with_logits\nor torch.nn.BCEWithLogitsLoss.  binary_cross_entropy_with_logits and BCEWithLogits are\nsafe to autocast."
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"  # 在main函数开头添加\n",
    "def main():\n",
    "    # 初始化配置\n",
    "    config = Config()\n",
    "    print(f\"Using device: {config.device}\")\n",
    "    \n",
    "    # 数据处理\n",
    "    processor = DataProcessor(config)\n",
    "    X, C = processor.load_and_process()\n",
    "    dataset = TensorDataset(X, C)\n",
    "    dataloader = DataLoader(dataset, batch_size=config.batch_size, shuffle=True)\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = TimeGAN(config)\n",
    "    \n",
    "    # 训练阶段\n",
    "    print(\"Starting Embedding Training...\")\n",
    "    embed_loss = train_embedding(model, dataloader, config)\n",
    "    \n",
    "    print(\"\\nStarting Supervisor Training...\")\n",
    "    super_loss = train_supervisor(model, dataloader, config)\n",
    "    \n",
    "    print(\"\\nStarting Joint Training...\")\n",
    "    joint_history = train_joint(model, dataloader, config)\n",
    "    \n",
    "    # 可视化损失\n",
    "    plot_losses(embed_loss, super_loss, joint_history, config)\n",
    "    \n",
    "    # 保存模型\n",
    "    torch.save({\n",
    "        'embedder': model.embedder.state_dict(),\n",
    "        'recovery': model.recovery.state_dict(),\n",
    "        'generator': model.generator.state_dict(),\n",
    "        'discriminator': model.discriminator.state_dict(),\n",
    "        'supervisor': model.supervisor.state_dict(),\n",
    "        'config': config\n",
    "    }, os.path.join(config.output_dir, \"timegan_model.pth\"))\n",
    "    \n",
    "    print(\"Training completed and model saved!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # CUDA断言错误检查\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0593b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79376a0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
